{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumptions & Simplifications (Hackathon Context):**\n",
    "-   **MindsDB is Mocked:** Interactions for maintenance logs and operational context are simulated.\n",
    "-   **Data Source:** We use `attack_z.csv` to get samples, assuming it represents incoming pre-processed data.\n",
    "-   **Focus:** Demonstrate model integration with CrewAI for an end-to-end (simulated) flow.\n",
    "\n",
    "## Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTHROPIC_API_KEY found in environment. Starts with: sk-ant-api...\n",
      "Anthropic Claude LLM (claude-3-haiku-20240307) initialized.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- LLM Configuration for Anthropic Claude ---\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "import os # Ensure os is imported if not already in this cell or a prior one that's run\n",
    "\n",
    "anthropic_api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "if anthropic_api_key:\n",
    "    print(f\"ANTHROPIC_API_KEY found in environment. Starts with: {anthropic_api_key[:10]}...\")\n",
    "    # You can choose other Claude models like claude-3-sonnet-20240229 or claude-3-opus-20240229\n",
    "    # Haiku is generally faster and more cost-effective for many tasks.\n",
    "    llm = ChatAnthropic(model_name=\"claude-3-haiku-20240307\", anthropic_api_key=anthropic_api_key, temperature=0.7)\n",
    "    print(\"Anthropic Claude LLM (claude-3-haiku-20240307) initialized.\")\n",
    "else:\n",
    "    print(\"ANTHROPIC_API_KEY NOT FOUND in environment. Please add it to your .env file (e.g., ANTHROPIC_API_KEY='your_key').\")\n",
    "    print(\"The agent will likely fail or attempt to use a default LLM if not configured.\")\n",
    "    llm = None # Set llm to None if key is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load .env file from: /Users/tim/myhub.com/playground/infra-triage/.env\n",
      ".env file loaded.\n",
      "ANTHROPIC_API_KEY found in environment. Starts with: sk-ant-api...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd # Keep other necessary imports too\n",
    "\n",
    "# --- Environment Setup ---\n",
    "# Construct the path to the .env file in the project root\n",
    "# Assumes this notebook is in a subdirectory (e.g., 'notebooks') of the project root\n",
    "\n",
    "try:\n",
    "    current_notebook_dir = os.getcwd() # Should be '.../infra-triage/notebooks'\n",
    "    project_root = os.path.abspath(os.path.join(current_notebook_dir, '..')) # Goes up to '.../infra-triage'\n",
    "    dotenv_path = os.path.join(project_root, '.env')\n",
    "\n",
    "    print(f\"Attempting to load .env file from: {dotenv_path}\")\n",
    "    if os.path.exists(dotenv_path):\n",
    "        # override=True ensures it reloads if you modify .env and re-run the cell\n",
    "        load_dotenv(dotenv_path=dotenv_path, override=True)\n",
    "        print(\".env file loaded.\")\n",
    "\n",
    "        # Test if the API key is loaded into the environment\n",
    "        openai_api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "        if openai_api_key:\n",
    "            print(f\"ANTHROPIC_API_KEY found in environment. Starts with: {openai_api_key[:10]}...\")\n",
    "        else:\n",
    "            print(\"ANTHROPIC_API_KEY NOT FOUND in environment after loading .env!\")\n",
    "    else:\n",
    "        print(f\".env file NOT FOUND at {dotenv_path}. Please ensure it exists there.\")\n",
    "        print(\"ANTHROPIC_API_KEY will not be loaded from this file.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during .env loading: {e}\")\n",
    "\n",
    "# --- REST OF YOUR IMPORTS AND SETUP CODE (like loading anomaly_model, attack_z_df, etc.) ---\n",
    "# Make sure all CrewAI agent, task, and crew definitions happen AFTER this .env loading block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTHROPIC_API_KEY confirmed available for LLM setup. Starts with: sk-ant-api...\n",
      "Anthropic Claude LLM (claude-3-haiku-20240307) initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- LLM Configuration for Anthropic Claude ---\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "import os\n",
    "\n",
    "retrieved_anthropic_api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if retrieved_anthropic_api_key:\n",
    "    print(f\"ANTHROPIC_API_KEY confirmed available for LLM setup. Starts with: {retrieved_anthropic_api_key[:10]}...\")\n",
    "\n",
    "    # Define the model name\n",
    "    anthropic_model_name = \"claude-3-haiku-20240307\" # You can change this to other Claude models\n",
    "\n",
    "    llm = ChatAnthropic(\n",
    "        model=anthropic_model_name, # Use 'model' parameter as per newer Langchain/Anthropic usage\n",
    "        anthropic_api_key=retrieved_anthropic_api_key,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    # For printing, we use the variable we defined, as direct attribute access like .model_name might vary\n",
    "    print(f\"Anthropic Claude LLM ({anthropic_model_name}) initialized successfully.\")\n",
    "else:\n",
    "    print(\"ERROR: ANTHROPIC_API_KEY was NOT found in os.environ at the point of LLM setup.\")\n",
    "    print(\"Please ensure the .env loading cell was run successfully before this cell.\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.tools import tool # Ensure this is imported if not done in Cell 3\n",
    "\n",
    "mock_maintenance_knowledge = {\n",
    "    \"LIT101\": \"Recent maintenance on LIT101 involved a sensor calibration completed 2 hours before the anomaly. Sensor readings might still be settling. Monitor for stability.\",\n",
    "    \"P101\": \"No recent maintenance scheduled or performed on P101 pump.\",\n",
    "    \"FIT201\": \"FIT201 underwent a firmware update yesterday. Unlikely related to immediate operational anomalies unless the update failed.\",\n",
    "    \"AIT501\": \"Scheduled cleaning for AIT501 is due next week. No active maintenance.\",\n",
    "    \"MV501\": \"MV501 valve actuator was inspected this morning; no issues reported. Normal operation expected.\",\n",
    "    \"PIT501\": \"PIT501 pressure sensor was replaced 3 days ago due to erratic readings. New sensor should be stable.\"\n",
    "}\n",
    "\n",
    "mock_operational_knowledge = {\n",
    "    \"LIT101\": \"LIT101 (Tank Level - Process 1) is critical. Sustained anomalies can lead to overflow or dry run. Brief spikes post-calibration are sometimes observed but should stabilize. Cross-reference with P101 and MV101.\",\n",
    "    \"P101\": \"P101 is a primary raw water pump (Process 1). Unexpected stoppage or erratic behavior is high-priority. Check power and downstream flow (FIT101/FIT201).\",\n",
    "    \"FIT201\": \"FIT201 (Flow Rate - Process 2) anomalies can indicate pump issues, blockages, or leaks. Correlate with LIT101/LIT301.\",\n",
    "    \"AIT501\": \"AIT501 measures water quality (e.g., conductivity - Process 5). Gradual drifts might indicate sensor fouling. Sudden changes can signify contamination.\",\n",
    "    \"MV501\": \"MV501 is a motorized valve in Process 5. Failure can disrupt flow. Check actuator and feedback.\",\n",
    "    \"PIT501\": \"PIT501 measures pressure in Process 5. Abnormal pressure can indicate blockages, leaks, or pump failures. Correlated with P501.\"\n",
    "}\n",
    "\n",
    "@tool(\"Maintenance Log Checker\")\n",
    "def maintenance_tool(sensor_id: str) -> str:\n",
    "    \"\"\"Checks and returns maintenance log summary for a given SCADA sensor ID. Input should be the sensor_id string.\"\"\"\n",
    "    print(f\"\\\\n<Tool Call: Maintenance Log Checker(sensor_id='{sensor_id}')>\")\n",
    "    response = mock_maintenance_knowledge.get(sensor_id, f\"No specific recent maintenance information found for sensor {sensor_id}.\")\n",
    "    print(f\"<Tool Response: {response}>\\\\n\")\n",
    "    return response\n",
    "\n",
    "@tool(\"Operational Context Retriever\")\n",
    "def operational_context_tool(sensor_id: str) -> str:\n",
    "    \"\"\"Retrieves and returns operational context, known behaviors, and potential implications for a given SCADA sensor ID. Input should be the sensor_id string.\"\"\"\n",
    "    print(f\"\\\\n<Tool Call: Operational Context Retriever(sensor_id='{sensor_id}')>\")\n",
    "    response = mock_operational_knowledge.get(sensor_id, f\"No specific operational context available for sensor {sensor_id}.\")\n",
    "    print(f\"<Tool Response: {response}>\\\\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Anomaly Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /Users/tim/myhub.com/playground/infra-triage/models/iforest_swat_model.pkl\n",
      "Anomaly detection model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(project_root, 'models', 'iforest_swat_model.pkl')\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "try:\n",
    "    anomaly_model = joblib.load(model_path)\n",
    "    print(\"Anomaly detection model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Model file not found at {model_path}. Please ensure '02_model_training.ipynb' was run and the model was saved.\")\n",
    "    anomaly_model = None\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load the model. {e}\")\n",
    "    anomaly_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SCADA Triage Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_triage_specialist = Agent(\n",
    "    role='SCADA Anomaly Triage Specialist',\n",
    "    goal=\"Analyze SCADA sensor anomalies and provide Assessment and RecommendedAction.\",\n",
    "    backstory=\"You are an expert SCADA analyst who determines if anomalies need urgent attention.\",\n",
    "    tools=[maintenance_tool, operational_context_tool],\n",
    "    llm=llm,\n",
    "    verbose=False,  # Changed to False to prevent recursion\n",
    "    allow_delegation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Triage Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "triage_task_definition = Task(\n",
    "    description=\"\"\"\n",
    "    Analyze this anomaly: {anomaly_details}\n",
    "\n",
    "    1. Check maintenance history using the Maintenance Log Checker tool\n",
    "    2. If needed, get operational context using the Operational Context Retriever tool\n",
    "    3. Provide your analysis in this exact format:\n",
    "\n",
    "    Assessment: [Your assessment]\n",
    "    RecommendedAction: [Your recommendation]\n",
    "    \"\"\",\n",
    "    expected_output=\"Assessment: [analysis]\\nRecommendedAction: [recommendation]\",\n",
    "    agent=scada_triage_specialist\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "scada_crew = Crew(\n",
    "    agents=[scada_triage_specialist],\n",
    "    tasks=[triage_task_definition],\n",
    "    process=Process.sequential,\n",
    "    verbose=False  # Changed to False to prevent recursion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data Sample with Model and Trigger Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Define the main simulation loop and supporting functions\n",
    "\n",
    "def process_data_and_trigger_crew(data_sample_series, data_timestamp, model, crew, all_sensor_columns):\n",
    "    \"\"\"\n",
    "    Processes a single data sample using the anomaly detection model\n",
    "    and triggers the CrewAI crew if an anomaly is detected.\n",
    "\n",
    "    Args:\n",
    "        data_sample_series (pd.Series): A single row of sensor readings (features only).\n",
    "        data_timestamp (pd.Timestamp): The timestamp of the data sample.\n",
    "        model: The trained anomaly detection model.\n",
    "        crew: The CrewAI crew instance.\n",
    "        all_sensor_columns (list): List of all sensor column names model was trained on.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        print(\"Model not loaded. Skipping prediction.\")\n",
    "        return\n",
    "\n",
    "    # Ensure the sample is a DataFrame with the correct columns for the model\n",
    "    sample_df = pd.DataFrame([data_sample_series], columns=all_sensor_columns)\n",
    "\n",
    "    # Get raw anomaly score and prediction\n",
    "    # Model predicts 1 for anomaly, 0 for normal (based on PyOD's IsolationForest if 'predict' is used)\n",
    "    # For IsolationForest, lower decision_function scores are typically more anomalous.\n",
    "    # The model.threshold_ is used by .predict() to make the 0/1 classification.\n",
    "    # A score > model.threshold_ means it's classified as an anomaly (1) by predict().\n",
    "\n",
    "    decision_scores = model.decision_function(sample_df) # Get the raw scores\n",
    "    prediction = model.predict(sample_df) # Get 0 or 1 prediction\n",
    "\n",
    "    print(f\"\\nProcessing sample from Timestamp: {data_timestamp}\")\n",
    "    print(f\"Raw Anomaly Score: {decision_scores[0]:.4f}\") # Print the score\n",
    "\n",
    "    if prediction[0] == 1: # Check if it's an anomaly (PyOD: 1 for anomaly)\n",
    "        print(f\"ALERT: Anomaly DETECTED by model at {data_timestamp}!\")\n",
    "\n",
    "        # Determine the primarily affected sensor for reporting (simplified heuristic)\n",
    "        anomalous_sensor_id = None\n",
    "        # Option 1: Check against mock data keys\n",
    "        for col in sample_df.columns:\n",
    "            if col in mock_maintenance_knowledge: # or mock_operational_knowledge\n",
    "                anomalous_sensor_id = col\n",
    "                break # Found one, use it\n",
    "\n",
    "        # Option 2: If none from mock data, pick one with max absolute Z-score (assuming data is Z-scaled)\n",
    "        if not anomalous_sensor_id:\n",
    "            anomalous_sensor_id = sample_df.abs().idxmax(axis=1).iloc[0]\n",
    "\n",
    "        # Option 3: Fallback (if all Z-scores are 0 or columns are empty)\n",
    "        if not anomalous_sensor_id and len(sample_df.columns) > 0:\n",
    "             anomalous_sensor_id = sample_df.columns[0]\n",
    "\n",
    "        if not anomalous_sensor_id : # Should not happen if sample_df has columns\n",
    "            print(\"Could not determine a specific sensor_id for the anomaly. Skipping crew kickoff.\")\n",
    "            return\n",
    "\n",
    "        reported_value = sample_df.iloc[0][anomalous_sensor_id]\n",
    "\n",
    "        # Prepare the string that will fill the {anomaly_details} placeholder in the Task description\n",
    "        anomaly_details_str_for_task = (\n",
    "            f\"SensorID: {anomalous_sensor_id}, \"\n",
    "            f\"Timestamp: {str(data_timestamp)}, \"\n",
    "            f\"ReportedValue: {reported_value:.4f}, \" # Added formatting\n",
    "            f\"RawScore: {decision_scores[0]:.4f}\" # Added RawScore for context\n",
    "        )\n",
    "        print(f\"Anomaly Details for Crew: {anomaly_details_str_for_task}\")\n",
    "\n",
    "        try:\n",
    "            # The key in the inputs dictionary MUST match the placeholder in the Task's description\n",
    "            # Task description has \"{anomaly_details}\", so the key here must be \"anomaly_details\"\n",
    "            print(f\"Kicking off crew with inputs: {{'anomaly_details': '{anomaly_details_str_for_task}'}}\")\n",
    "            result = crew.kickoff(inputs={'anomaly_details': anomaly_details_str_for_task})\n",
    "\n",
    "            print(\"\\n--- CrewAI Triage Result ---\")\n",
    "            print(result)\n",
    "            print(\"-----------------------------\")\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during crew kickoff: {e}\")\n",
    "            # You might want to print more details if the error is complex\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(f\"Status: Data sample at {data_timestamp} classified as NORMAL by the model.\")\n",
    "\n",
    "# --- Your main simulation loop (Cell 8 continued) ---\n",
    "# This should come AFTER the function definition above in the same cell, or in a subsequent cell.\n",
    "# Example:\n",
    "# print(\"\\n--- Starting Simulation with Integrated Model ---\")\n",
    "# ... (load anomaly_model, full_data_df, model_feature_columns) ...\n",
    "# sample_indices_to_test = [1, 1754, 3150, 3151] # Your example indices\n",
    "\n",
    "# for i in sample_indices_to_test:\n",
    "#    if i < len(full_data_df):\n",
    "#        sample_series_features_only = full_data_df.iloc[i][model_feature_columns]\n",
    "#        sample_timestamp = full_data_df.index[i]\n",
    "#        process_data_and_trigger_crew(sample_series_features_only, sample_timestamp, anomaly_model, scada_crew, model_feature_columns)\n",
    "# print(\"\\n--- Simulation Ended ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation: Load Data, Feed to Model, and Run Triage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Starting Simulation with Integrated Model ---\n",
      "Loading data for simulation from: /Users/tim/myhub.com/playground/infra-triage/data/processed/attack_z.csv\n",
      "Model expects 34 features. First few: ['FIT101', 'LIT101', 'MV101', 'P101', 'P102']\n",
      "\n",
      "Processing sample from Timestamp: 2015-12-28 10:00:01\n",
      "Raw Anomaly Score: -0.2519\n",
      "Status: Data sample at 2015-12-28 10:00:01 classified as NORMAL by the model.\n",
      "\n",
      "Processing sample from Timestamp: 2015-12-28 10:29:14\n",
      "Raw Anomaly Score: -0.0906\n",
      "Status: Data sample at 2015-12-28 10:29:14 classified as NORMAL by the model.\n",
      "\n",
      "Processing sample from Timestamp: 2015-12-28 10:52:30\n",
      "Raw Anomaly Score: 0.0110\n",
      "ALERT: Anomaly DETECTED by model at 2015-12-28 10:52:30!\n",
      "Anomaly Details for Crew: SensorID: LIT101, Timestamp: 2015-12-28 10:52:30, ReportedValue: 0.4591, RawScore: 0.0110\n",
      "Kicking off crew with inputs: {'anomaly_details': 'SensorID: LIT101, Timestamp: 2015-12-28 10:52:30, ReportedValue: 0.4591, RawScore: 0.0110'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/swat/lib/python3.10/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but IsolationForest was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/swat/lib/python3.10/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but IsolationForest was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/swat/lib/python3.10/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but IsolationForest was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/swat/lib/python3.10/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but IsolationForest was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/swat/lib/python3.10/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but IsolationForest was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/swat/lib/python3.10/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but IsolationForest was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n<Tool Call: Maintenance Log Checker(sensor_id='LIT101')>\n",
      "<Tool Response: Recent maintenance on LIT101 involved a sensor calibration completed 2 hours before the anomaly. Sensor readings might still be settling. Monitor for stability.>\\n\n",
      "\\n<Tool Call: Operational Context Retriever(sensor_id='LIT101')>\n",
      "<Tool Response: LIT101 (Tank Level - Process 1) is critical. Sustained anomalies can lead to overflow or dry run. Brief spikes post-calibration are sometimes observed but should stabilize. Cross-reference with P101 and MV101.>\\n\n",
      "\n",
      "--- CrewAI Triage Result ---\n",
      "Assessment: The reported anomaly for LIT101 is likely a temporary spike following a recent sensor calibration. Brief fluctuations are expected as the sensor readings settle. However, the sensor is critical, so the anomaly should be monitored closely.\n",
      "RecommendedAction: Monitor the LIT101 sensor readings over the next 2-4 hours. Verify that the readings stabilize within the expected operational range by cross-referencing with related sensors P101 and MV101. If the anomaly persists or worsens, escalate for further investigation.\n",
      "-----------------------------\n",
      "\n",
      "Processing sample from Timestamp: 2015-12-28 10:52:31\n",
      "Raw Anomaly Score: 0.0138\n",
      "ALERT: Anomaly DETECTED by model at 2015-12-28 10:52:31!\n",
      "Anomaly Details for Crew: SensorID: LIT101, Timestamp: 2015-12-28 10:52:31, ReportedValue: 0.4565, RawScore: 0.0138\n",
      "Kicking off crew with inputs: {'anomaly_details': 'SensorID: LIT101, Timestamp: 2015-12-28 10:52:31, ReportedValue: 0.4565, RawScore: 0.0138'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/swat/lib/python3.10/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but IsolationForest was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/swat/lib/python3.10/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but IsolationForest was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CrewAI Triage Result ---\n",
      "Assessment: The anomaly for sensor LIT101 is likely due to the sensor still settling after a recent calibration. Brief spikes in the sensor reading are sometimes observed after calibration, but the readings should stabilize within a short period of time.\n",
      "\n",
      "RecommendedAction: Monitor the sensor readings for LIT101 over the next few hours to ensure the readings stabilize. Cross-reference the LIT101 readings with the readings from related sensors P101 and MV101 to verify the overall system behavior. If the readings do not stabilize within a reasonable timeframe or if there are any other indications of a more serious issue, further investigation may be warranted.\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "if anomaly_model:\n",
    "    data_csv_path = os.path.join(project_root, 'data', 'processed', 'attack_z.csv')\n",
    "    print(f\"\\\\n--- Starting Simulation with Integrated Model ---\")\n",
    "    print(f\"Loading data for simulation from: {data_csv_path}\")\n",
    "\n",
    "    try:\n",
    "        full_data_df = pd.read_csv(data_csv_path, index_col=0, parse_dates=True)\n",
    "\n",
    "        # Get the feature columns the model was trained on (excluding 'Normal/Attack' if present)\n",
    "        # The loaded IsolationForest model from PyOD doesn't store feature names directly in a simple attribute.\n",
    "        # We assume the columns in normal_z.csv/attack_z.csv (excluding 'Normal/Attack') are the correct ones.\n",
    "        # If 'Normal/Attack' is in columns, drop it for prediction features.\n",
    "        model_feature_columns = [col for col in full_data_df.columns if col != 'Normal/Attack']\n",
    "\n",
    "        if not model_feature_columns:\n",
    "             print(\"ERROR: Could not determine model feature columns from the loaded CSV.\")\n",
    "        else:\n",
    "            print(f\"Model expects {len(model_feature_columns)} features. First few: {model_feature_columns[:5]}\")\n",
    "\n",
    "            # Simulate processing a few samples\n",
    "            # For a more diverse test, you might pick specific indices:\n",
    "            # some known attacks, some known normal periods from your 'normal_z.csv'\n",
    "\n",
    "            # Example: Process first 2 samples from attack_z.csv (which are likely attacks)\n",
    "            # and one sample much later that might be different.\n",
    "            # sample_indices_to_test = [1754, 1755, 1756, 1757] # attack_z.csv has attacks, good for testing 'anomaly' path\n",
    "            sample_indices_to_test = [1, 1754, 3150, 3151]\n",
    "            # If you also want to test 'normal' predictions, load 'normal_z.csv'\n",
    "            # normal_data_csv_path = os.path.join(project_root, 'data', 'processed', 'normal_z.csv')\n",
    "            # normal_df = pd.read_csv(normal_data_csv_path, index_col=0, parse_dates=True)\n",
    "            # sample_indices_to_test.extend([len(full_data_df) + i for i in range(2)]) # Placeholder for normal samples\n",
    "            # combined_df_for_testing = pd.concat([full_data_df, normal_df.iloc[:2]]) # Example only\n",
    "\n",
    "            for i in sample_indices_to_test:\n",
    "                if i < len(full_data_df):\n",
    "                    sample_series_features_only = full_data_df.iloc[i][model_feature_columns]\n",
    "\n",
    "                    sample_timestamp = full_data_df.index[i]\n",
    "                    process_data_and_trigger_crew(sample_series_features_only, sample_timestamp, anomaly_model, scada_crew, model_feature_columns)\n",
    "                else:\n",
    "                    # Example for normal data if you were to load and append it\n",
    "                    # normal_sample_idx = i - len(full_data_df)\n",
    "                    # sample_series_features_only = normal_df.iloc[normal_sample_idx][model_feature_columns]\n",
    "                    # sample_timestamp = normal_df.index[normal_sample_idx]\n",
    "                    # process_data_and_trigger_crew(sample_series_features_only, sample_timestamp, anomaly_model, scada_crew, model_feature_columns)\n",
    "                    pass # Not processing normal samples in this specific example loop\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Data CSV file not found at {data_csv_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during simulation: {e}\")\n",
    "else:\n",
    "    print(\"Simulation cannot run because the anomaly model was not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load attack data from: /Users/tim/myhub.com/playground/infra-triage/data/processed/attack_z.csv\n",
      "Attack data loaded successfully. Shape: (449919, 35)\n",
      "\n",
      "--- Ground Truth for Sample Indices ---\n",
      "Timestamp\n",
      "2015-12-28 10:00:01    Normal\n",
      "2015-12-28 10:29:14    Attack\n",
      "2015-12-28 10:52:30    Attack\n",
      "2015-12-28 10:52:31    Attack\n",
      "Name: Normal/Attack, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os # Often useful for path manipulation\n",
    "\n",
    "# --- Load Data ---\n",
    "# Construct the path to the data file relative to the project root\n",
    "# Assuming your notebook is in 'notebooks/' and data is in 'data/processed/'\n",
    "# Adjust if your notebook structure is different.\n",
    "\n",
    "# Get the absolute path of the current notebook (or script)\n",
    "try:\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__)) # For .py scripts\n",
    "except NameError:\n",
    "    current_dir = os.getcwd() # For .ipynb notebooks\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..')) # Go up one level from 'notebooks' to project root\n",
    "attack_data_path = os.path.join(project_root, 'data', 'processed', 'attack_z.csv')\n",
    "\n",
    "print(f\"Attempting to load attack data from: {attack_data_path}\")\n",
    "\n",
    "try:\n",
    "    attack_z_df = pd.read_csv(attack_data_path, index_col=0, parse_dates=True)\n",
    "    print(f\"Attack data loaded successfully. Shape: {attack_z_df.shape}\")\n",
    "    # Display the first few rows to confirm\n",
    "    # display(attack_z_df.head()) # Use display() in Jupyter, print() otherwise\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: File not found at {attack_data_path}\")\n",
    "    print(\"Please ensure the path is correct and the file exists.\")\n",
    "    attack_z_df = None # Set to None if not loaded\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading attack_z.csv: {e}\")\n",
    "    attack_z_df = None\n",
    "\n",
    "# Now you can define your sample_indices_to_test\n",
    "# sample_indices_to_test = [1754, 1755, 1756, 1757] # Or any other indices you want to check\n",
    "sample_indices_to_test = [1, 1754, 3150, 3151]\n",
    "\n",
    "# --- Ground Truth Check (can now run if attack_z_df loaded) ---\n",
    "print(\"\\n--- Ground Truth for Sample Indices ---\")\n",
    "if attack_z_df is not None:\n",
    "    try:\n",
    "        ground_truths = attack_z_df.iloc[sample_indices_to_test]['Normal/Attack']\n",
    "        print(ground_truths)\n",
    "        # For better readability with the data:\n",
    "        # print(\"\\nSelected rows with ground truth:\")\n",
    "        # display(attack_z_df.iloc[sample_indices_to_test]) # Shows the actual data for these rows\n",
    "    except IndexError:\n",
    "        print(f\"Error: One or more indices in {sample_indices_to_test} are out of bounds for attack_z_df (rows: {len(attack_z_df)}).\")\n",
    "    except KeyError:\n",
    "        print(\"Error: 'Normal/Attack' column not found in attack_z_df.\")\n",
    "else:\n",
    "    print(\"attack_z_df was not loaded, cannot check ground truths.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
